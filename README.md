#  word2vector

word embedding methods using CBOW，skip-Gram，word2doc matrix , word2word matrix  
基于CBOW、skip-gram、词-文档矩阵、词-词矩阵四种方法的词向量生成  

# How to run?

```shell
bash run.sh
```

This command will create the environment that needed by the models.  
This project is created on the purposes of easy-on-run.  
If you want to know the details about the models, just read code.  

# 简介：   

Harris 在1954 年提出的分布假说（distributional hypothesis）为这一设想提供了理论基础：上下文相似的词，其语义也相似。  
Firth 在1957 年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定（a word is characterized by thecompany it keeps）。  
基于分布假说得到的表示均可称为分布表示（distributional representation），它们的核心思想也都由两部分组成：  
1）选择一种方式描述上下文；  
2）选择一种模型刻画某个词（下文称“目标词”）与其上下文之间的关系。  
根据建模的不同，主要可以分为三类：  
1）基于矩阵的分布表示  
这类方法需要构建一个“词-上下文”矩阵，从矩阵中获取词的表示。在“词-上下文”矩阵中，每行对应一个词，每列表示一种不同的上下文，矩阵中的每个元素对应相关词和上下文的共现次数。  
在这种表示下，矩阵中的一行，就成为了对应词的表示，这种表示描述了该词的上下文的分布。  
2）基于聚类的分布表示  
该方法以根据两个词的公共类别判断这两个词的语义相似度。最经典的方法是布朗聚类（Brown clustering）  
3）基于神经网络的分布表示  
基于神经网络的分布表示一般称作词向量、 词嵌入（word embedding）、分布式表示（distributed representation）  

# 项目介绍  

本项目主要实现  
1）基于矩阵的分布式表示  
2）基于神经网络的分布表示两种方法。  

其中  
1）包括词-文档共现矩阵，词-词共现矩阵两种。  
2）包括cbow和skipgram两种模型。    

# 训练过程

一、输入语料：  
1、经过分词的新闻文本共1W篇，约30M，详见data/data.txt  
2、使用词-文档共现方法构建词向量，详见word2doc.py  
3、使用词-词共现方法构建词向量，详见word2word.py  
4、使用cbow方法构建词向量，详见cbow.py  
5、使用skip-gram方法构建词向量，详见skipgram.py  
二、参数设置：  
1、window_size:上下文窗口，默认设置为5，即前后5个词，这个在词-词共现以及cbow,skipgram中会用到  
2、min_count:词语最低频次阈值，减少计算量，忽略稀有词，设定阈值。  
3、embedding_size:生成的词向量维度，默认设置为200  
三、模型保存  
模型保存在model当中，因为github对文件大小有限制，未能上传  
四、效果对比  
1、背景：因计算量较大，设备不支持。词-词共现方法使用1000篇文档进行训练(内存限制），其余使用全部语料。cbow和sikpgram分别迭代训练10W次  
2、效果评价：    

运行run.sh,通过加载相应模型，进行词语聚类，可以验证词向量效果：  
举例如下:以'中国','美国',‘教育’、‘疾病’、‘母亲’三个为例，分别返回相应词向量得到的最相近词TOP10：  

中国  
word2word output: '重拾', '习性', '工业生产', '低俗', '菊花', '高消费', '超级大国', '输出', '犹存', '封建制度',  
word2doc output: '旅游', '美国', '印度', '日本', '公司', '发展', '企业', '集团', '北京', '国家',  
skipgram output: '美国', '发展', '日本', '底物', '世界', '280圆', '市场', '茴香', '经线', '鸡杂',  
cbow output: '留学生', '象样', '钠离子', '我国', '社会主义国家', '凯旋', '红一方面军', '准噶尔', '古代文学', '榜单',  

美国  
word2word output: '低俗', '重拾', '贪渎', '楚文化', '高消费', '狂欢', '习性', '遗存', '超级大国', '解析',  
word2doc output: '中国', '美元', '签证', '博士伦', '日本', '护理', '国家', '印度', '人民币', '汇率',  
skipgram output: '中国', '日本', '280圆', '底物', '道统', '茴香', '经线', '发现', '奥运项目', '这是',  
cbow output: '花卉市场', '违约', '号令', '意大利', '领到', '军费', '个儿', '流动性', '英国', '养尊处优',  

教育  
word2word output: '威胁论', '教育产业化', '贫困家庭', '中国制造', '节庆', '町村', '课上', '一盘散沙', '中文系', '精神教育',  
word2doc output: '学校', '学生', '孩子', '教师', '招生', '考试', '家长', '考生', '高校', '课程',  
skipgram output: '社会', '美国', '中国', '国家', '发展', '这一', '学校', '我国', '经济', '2005-11-13',  
cbow output: '行政法', '新教材', '种树', '内城', '沾上', '执政', '精神力量', '供人', '待定', '过活',  

疾病  
word2word output: '生殖', '胚胎干细胞', '不具', '克隆技术', '克隆', '长势', '克隆人', '胚胎', '诊疗', '法律委员会',  
word2doc output: '治疗', 'gt', '患者', '高血压', '病人', '哮喘', '药物', '女性', '症状', '医院',  
skipgram output: '1365672', '铁托', '东莞市', 'occasion', '经线', '0.08', '用典', '风月', '艾滋病防治条例', '碱基对',  
cbow output: '癌症', '高血压', '总工会', '呜咽', '红客', '婴儿', '中国科协', '复习资料', '移植', '病人',  

母亲  
word2word output: '亲生', '球星', '歌星', '肩挑', '勇敢地', '操心', '寻医问药', '母爱', '赶去', '巴桑',  
word2doc output: '孩子', 'newwin', 'url', '婴儿', '皇后', '乳腺癌', '父母', '治疗', '家长', '疫苗',  
skipgram output: '铁锨', '书香', 'polo', '0533', '清净剂', '于我', '升官', '发行日期', '奶子', '每题',  
cbow output: '艺人', '自觉性', '皇后', '听谁', '提及', '结果却', '厂址', '活起来', '秋山', '了得',  

# 总结  
1、基于共现矩阵分解的词向量构建方法对硬件要求较高，会随着词数和文档数量迅速增加  
2、虽然word2word采用的是1k的语料，是woed2doc的十分之一，(内存限制)但从效果上来看，还行，说明word2word的方法与word2doc相比如何?  
3、从当前结果上来看，小语料当中基于矩阵的分布式表示效果要好于基于神经网络的分布表示  
4、从cbow和skipgram的效果看来，cbow在小语料当中表现要更好  
5、算法还有许多不足，需要继续改进  
